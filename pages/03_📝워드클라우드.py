import streamlit as st
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from urllib.parse import urlparse, parse_qs
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from io import BytesIO

# -----------------------------
# ğŸ” ë””ë²„ê·¸ìš©: ì§€ê¸ˆ ì´ ì•±ì—ì„œ ì½íˆëŠ” secrets í‚¤ë“¤ í™•ì¸
# -----------------------------
st.sidebar.write("ğŸ” Secrets keys:", list(st.secrets.keys()))


# -----------------------------
# 1. ìœ íŠœë¸Œ ì˜ìƒ ID ì¶”ì¶œ í•¨ìˆ˜
# -----------------------------
def extract_video_id(url):
    try:
        parsed = urlparse(url)
        if parsed.hostname in ["youtu.be"]:
            return parsed.path[1:]
        if parsed.hostname in ["www.youtube.com", "youtube.com"]:
            return parse_qs(parsed.query).get("v", [None])[0]
    except:
        return None


# -----------------------------
# 2. ëŒ“ê¸€ ë¶ˆëŸ¬ì˜¤ê¸°
# -----------------------------
def get_all_comments(api_key, video_id, max_pages=5):
    youtube = build("youtube", "v3", developerKey=api_key)

    comments = []
    page_token = None

    for _ in range(max_pages):
        try:
            request = youtube.commentThreads().list(
                part="snippet",
                videoId=video_id,
                maxResults=100,
                textFormat="plainText",
                pageToken=page_token,
            )
            response = request.execute()

        except HttpError as e:
            if e.resp.status == 403:
                raise RuntimeError("ì´ ì˜ìƒì€ ëŒ“ê¸€ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            raise

        for item in response.get("items", []):
            c = item["snippet"]["topLevelComment"]["snippet"]
            comments.append(c.get("textDisplay", ""))

        page_token = response.get("nextPageToken")
        if not page_token:
            break

    return comments


# -----------------------------
# Streamlit UI
# -----------------------------
st.title("ğŸŒˆ YouTube ëŒ“ê¸€ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±ê¸°")
st.write("ë§ì´ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ì¼ìˆ˜ë¡ í¬ê²Œ ë³´ì´ëŠ” ì‹œê°í™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤!")

# âœ… ë‹¤ë¥¸ í˜ì´ì§€ì™€ 'ì™„ì „íˆ ë˜‘ê°™ì´' API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
api_key = st.secrets.get("YT_API_KEY")
st.write("ğŸ” DEBUG - api_key is None? â†’", api_key is None)

youtube_url = st.text_input("ğŸ¥ YouTube ì˜ìƒ URL ì…ë ¥")
max_pages = st.slider("ê°€ì ¸ì˜¬ ëŒ“ê¸€ í˜ì´ì§€ ìˆ˜ (1í˜ì´ì§€=100ê°œ)", 1, 10, 5)

# -----------------------------
# ë²„íŠ¼ í´ë¦­ ì‹œ ì‹¤í–‰
# -----------------------------
if st.button("ì›Œë“œí´ë¼ìš°ë“œ ë§Œë“¤ê¸°"):
    if not api_key:
        st.error("âŒ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ ì•±ì˜ Secretsì— YT_API_KEYë¥¼ ë‹¤ì‹œ í™•ì¸í•´ ì£¼ì„¸ìš”.")
        st.stop()

    video_id = extract_video_id(youtube_url)
    if not video_id:
        st.error("âŒ ì˜¬ë°”ë¥¸ ìœ íŠœë¸Œ ë§í¬ê°€ ì•„ë‹™ë‹ˆë‹¤.")
        st.stop()

    try:
        with st.spinner("ëŒ“ê¸€ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            comments = get_all_comments(api_key, video_id, max_pages)

    except RuntimeError as e:
        st.error(str(e))
        st.stop()

    except Exception as e:
        st.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.stop()

    if not comments:
        st.warning("ëŒ“ê¸€ì´ í•˜ë‚˜ë„ ì—†ì–´ìš”!")
        st.stop()

    all_text = " ".join(comments)

    # Streamlit Cloudì—ì„œ í•œê¸€ ì§€ì›ë˜ëŠ” í°íŠ¸ (Noto)
    font_path = "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc"

    wc = WordCloud(
        font_path=font_path,
        width=800,
        height=400,
        background_color="white",
    ).generate(all_text)

    fig, ax = plt.subplots(figsize=(10, 6))
    ax.imshow(wc, interpolation="bilinear")
    ax.axis("off")
    st.pyplot(fig)

    img_bytes = BytesIO()
    fig.savefig(img_bytes, format="png")
    img_bytes.seek(0)

    st.download_button(
        label="ğŸ“¥ ì›Œë“œí´ë¼ìš°ë“œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ",
        data=img_bytes,
        file_name="wordcloud.png",
        mime="image/png",
    )

    st.success("ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì™„ë£Œ!")
