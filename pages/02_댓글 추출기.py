import streamlit as st
import requests
from urllib.parse import urlparse, parse_qs

st.set_page_config(page_title="ìœ íŠœë¸Œ ëŒ“ê¸€ ê²€ìƒ‰", page_icon="ğŸ”")

# âœ… YouTube API í‚¤ (secretsì— ë§ê²Œ ì´ë¦„ë§Œ ìˆ˜ì •í•´ì„œ ì‚¬ìš©!)
API_KEY = st.secrets["YOUTUBE_API_KEY"]  # ì˜ˆ: st.secrets["youtube"]["api_key"] ë¡œ ì¨ë„ ë¨


# ---------------------------
# ìœ íŠœë¸Œ ë§í¬ì—ì„œ videoId ë½‘ëŠ” í•¨ìˆ˜
# ---------------------------
def extract_video_id(url_or_id: str) -> str:
    """ìœ íŠœë¸Œ ì „ì²´ ë§í¬ / shorts ë§í¬ / live ë§í¬ / ê·¸ëƒ¥ videoId ëª¨ë‘ ì²˜ë¦¬"""
    text = url_or_id.strip()

    # ê·¸ëƒ¥ IDë§Œ ë„£ì€ ê²½ìš°ë„ í—ˆìš© (ê¸¸ì´ 11ì§œë¦¬ ë“±)
    if "youtube.com" not in text and "youtu.be" not in text:
        return text

    parsed = urlparse(text)

    # youtu.be/VIDEO_ID
    if parsed.hostname in ("youtu.be", "www.youtu.be"):
        return parsed.path.lstrip("/")

    # youtube.com/watch?v=VIDEO_ID
    if parsed.hostname and "youtube.com" in parsed.hostname:
        # /watch?v=VIDEO_ID
        if parsed.path == "/watch":
            qs = parse_qs(parsed.query)
            return qs.get("v", [""])[0]

        # /shorts/VIDEO_ID
        if parsed.path.startswith("/shorts/"):
            return parsed.path.split("/")[2]

        # /live/VIDEO_ID
        if parsed.path.startswith("/live/"):
            return parsed.path.split("/")[2]

    return ""


# ---------------------------
# ëŒ“ê¸€ ë°›ì•„ì˜¤ëŠ” í•¨ìˆ˜
# ---------------------------
@st.cache_data(show_spinner=False)
def fetch_all_comments(video_id: str, max_pages: int = 10):
    """í•´ë‹¹ video_idì˜ ìƒìœ„ ëŒ“ê¸€ë“¤ì„ ì—¬ëŸ¬ í˜ì´ì§€ì— ê±¸ì³ ê°€ì ¸ì˜¤ê¸°"""
    comments = []
    url = "https://www.googleapis.com/youtube/v3/commentThreads"

    params = {
        "key": API_KEY,
        "part": "snippet",
        "videoId": video_id,
        "maxResults": 100,
        "order": "relevance",      # í•„ìš”ì— ë”°ë¼ 'time' ìœ¼ë¡œ ë°”ê¿”ë„ ë¨
        "textFormat": "plainText",
    }

    page_count = 0

    while True:
        resp = requests.get(url, params=params)
        data = resp.json()

        # ì—ëŸ¬ ì²˜ë¦¬
        if "error" in data:
            raise RuntimeError(data["error"]["message"])

        for item in data.get("items", []):
            top = item["snippet"]["topLevelComment"]["snippet"]
            comments.append(
                {
                    "author": top.get("authorDisplayName", ""),
                    "text": top.get("textDisplay", ""),
                    "likeCount": top.get("likeCount", 0),
                    "publishedAt": top.get("publishedAt", ""),
                }
            )

        page_count += 1
        if "nextPageToken" not in data or page_count >= max_pages:
            break

        params["pageToken"] = data["nextPageToken"]

    return comments


# ---------------------------
# Streamlit UI
# ---------------------------
st.title("ğŸ” ìœ íŠœë¸Œ ëŒ“ê¸€ ê²€ìƒ‰ í˜ì´ì§€")

st.markdown(
    """
ìœ íŠœë¸Œ ë§í¬ì™€ **ê²€ìƒ‰ì–´**ë¥¼ ì…ë ¥í•˜ë©´  
ê·¸ ê²€ìƒ‰ì–´ê°€ ë“¤ì–´ê°„ ëŒ“ê¸€ë§Œ ê³¨ë¼ì„œ ë³´ì—¬ì¤„ê²Œ!
"""
)

col1, col2 = st.columns(2)
with col1:
    video_input = st.text_input(
        "ğŸ¥ ìœ íŠœë¸Œ ë§í¬ ë˜ëŠ” ì˜ìƒ ID",
        placeholder="ì˜ˆ: https://www.youtube.com/watch?v=XXXXXXXXXXX",
    )
with col2:
    keyword = st.text_input(
        "ğŸ” ê²€ìƒ‰í•  ë‹¨ì–´ë‚˜ ë¬¸ì¥",
        placeholder="ì˜ˆ: ì¬ë°Œì–´ìš”, ê³µê°, ë„ˆë¬´ ì¢‹ë‹¤",
    )

limit = st.slider(
    "ê°€ì ¸ì˜¬ ìµœëŒ€ ëŒ“ê¸€ í˜ì´ì§€ ìˆ˜ (1í˜ì´ì§€ = ìµœëŒ€ 100ê°œ ëŒ“ê¸€)",
    min_value=1,
    max_value=20,
    value=5,
    help="ë„ˆë¬´ ë§ì´ ê°€ì ¸ì˜¤ë©´ ëŠë ¤ì§ˆ ìˆ˜ ìˆì–´ìš”!",
)

if st.button("ëŒ“ê¸€ ê²€ìƒ‰í•˜ê¸°"):
    if not video_input.strip():
        st.warning("ë¨¼ì € ìœ íŠœë¸Œ ë§í¬(ë˜ëŠ” ì˜ìƒ ID)ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    elif not keyword.strip():
        st.warning("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    else:
        video_id = extract_video_id(video_input)

        if not video_id:
            st.error("ì˜ìƒ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”. ë§í¬ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ ì£¼ì„¸ìš”.")
        else:
            with st.spinner("ìœ íŠœë¸Œì—ì„œ ëŒ“ê¸€ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤... â³"):
                try:
                    comments = fetch_all_comments(video_id, max_pages=limit)
                except Exception as e:
                    st.error(f"ëŒ“ê¸€ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”: {e}")
                    comments = []

            if not comments:
                st.info("ê°€ì ¸ì˜¨ ëŒ“ê¸€ì´ ì—†ì–´ìš”.")
            else:
                # ğŸ” ê²€ìƒ‰ì–´ í•„í„°ë§ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
                key = keyword.lower()
                filtered = [
                    c for c in comments
                    if key in c["text"].lower()
                ]

                if not filtered:
                    st.info(f"'{keyword}' ê°€(ì´) ë“¤ì–´ê°„ ëŒ“ê¸€ì„ ì°¾ì§€ ëª»í–ˆì–´ìš”.")
                else:
                    # ì¢‹ì•„ìš” ë§ì€ ìˆœìœ¼ë¡œ ì •ë ¬
                    filtered.sort(key=lambda x: x["likeCount"], reverse=True)

                    st.success(
                        f"âœ… '{keyword}' ê°€(ì´) í¬í•¨ëœ ëŒ“ê¸€ {len(filtered)}ê°œë¥¼ ì°¾ì•˜ì–´ìš”!"
                    )

                    # ìƒë‹¨ ìš”ì•½
                    top_like = filtered[0]["likeCount"]
                    st.write(f"- ê°€ì¥ ì¢‹ì•„ìš”ê°€ ë§ì€ ëŒ“ê¸€ì˜ ì¢‹ì•„ìš” ìˆ˜: **{top_like}ê°œ**")

                    # ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
                    for c in filtered:
                        st.markdown("---")
                        st.markdown(f"**ì‘ì„±ì**: {c['author']}")
                        st.markdown(
                            f"ğŸ‘ ì¢‹ì•„ìš”: **{c['likeCount']}ê°œ**  |  â° {c['publishedAt']}"
                        )
                        st.write(c["text"])
